{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85998053-d7ae-412d-838d-ae444d227b9c",
   "metadata": {},
   "source": [
    "## DAT341 Applied Machine Learning\n",
    "\n",
    "#### Assginment 3: Stance classification\n",
    "\n",
    "First, let's import the datasets. There are two datasets which one of them is used for training and the other one is for testing. Meanwhile, we could print out part of the dataset to see the format of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd33faff-088c-43ac-a669-c9bf861f8ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample\n",
      "     label                                               text\n",
      "0      0/0  A woman's autonomous right to decide for her o...\n",
      "1     1/-1   And yet ignorance is an enemy, even to its owner\n",
      "2      0/0  Anti vaxxer??? So if I have had a tetanus shot...\n",
      "3      0/0  Are you planning on getting the vaccine when i...\n",
      "4  0/0/0/0  Benefits outweigh the risks. Yes, the benefits...\n",
      "Test sample\n",
      "   label                                               text\n",
      "0      0  Extremely rare is only good if it doesn't happ...\n",
      "1      1  I have two parents in their 70s. Both had the ...\n",
      "2      0  Not getting vaccinated is still more dangerous...\n",
      "3      1  The average life expectancy of a human is 74 y...\n",
      "4      1  Trust the science is a dumb saying. Science is...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file=\"a3_train_final.tsv\"\n",
    "test_file=\"a3_test.tsv\"\n",
    "\n",
    "df_train=pd.read_csv(train_file,sep=\"\\t\",header=None,names=[\"label\",\"text\"])\n",
    "df_test=pd.read_csv(test_file,sep=\"\\t\",header=None,names=[\"label\",\"text\"] )\n",
    "\n",
    "print(\"Train sample\")\n",
    "print(df_train.head())\n",
    "print(\"Test sample\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5fb25-abad-43d6-b0bf-932667341e45",
   "metadata": {},
   "source": [
    "Before we preprocess the data, we first need to handle the multi-annotation problem. Different people(2 or more) could give different remark to the same comment. To simplify the situation, we could take the average of multiple annotation values and rounding to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be3d4fcf-9c73-45d6-acb0-142666294e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    5889\n",
      "1    5707\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def resolve_labels(label_str):\n",
    "    labels = list(map(int, label_str.split('/')))  \n",
    "    labels = [0.5 if l == -1 else l for l in labels]  \n",
    "    return round(np.mean(labels))  \n",
    "\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(str).apply(resolve_labels)\n",
    "df_train = df_train.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "\n",
    "print(df_train[\"label\"].value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134defed-41a2-4ce5-b33c-0848ae8eba64",
   "metadata": {},
   "source": [
    "It seems like the nubmer of the annoation in '0' is similar to that in '1'. Thus, we could move on to the next part which we handle the text of the comments here. As is shown in the dataset and the common sense about remarks in Youtube or other platforms, there might be complex symbols and useless words in the comments. The text needs to be cleaned by removing punctuation, converting to lowercase, removing stopwords, eliminating extra spaces, and applying stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8ddbdc7-7d41-4a7f-92fb-3ead49e6aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Text Sample:\n",
      "   label                                               text  \\\n",
      "0      0  A woman's autonomous right to decide for her o...   \n",
      "1      1   And yet ignorance is an enemy, even to its owner   \n",
      "2      0  Anti vaxxer??? So if I have had a tetanus shot...   \n",
      "3      0  Are you planning on getting the vaccine when i...   \n",
      "4      0  Benefits outweigh the risks. Yes, the benefits...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  womans autonomous right decide body dont auton...  \n",
      "1                     yet ignorance enemy even owner  \n",
      "2  anti vaxxer tetanus shot typhoid fever hepatit...  \n",
      "3  planning getting vaccine available nope hopefu...  \n",
      "4  benefits outweigh risks yes benefits getting s...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xiach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  \n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    return \" \".join(words)\n",
    "\n",
    "df_train[\"clean_text\"] = df_train[\"text\"].apply(preprocess_text)\n",
    "df_test[\"clean_text\"] = df_test[\"text\"].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nProcessed Text Sample:\")\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d4752-b8f8-40ea-bb42-a22efb932442",
   "metadata": {},
   "source": [
    "Performing TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3037d0df-d0ed-4756-b5ed-c0506a22b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train[\"clean_text\"])\n",
    "y_train = df_train[\"label\"]\n",
    "\n",
    "X_test = vectorizer.transform(df_test[\"clean_text\"])\n",
    "y_test = df_test[\"label\"]\n",
    "\n",
    "print(\"\\nCompleted!!!!!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a584c21-1d46-4e9c-8a25-4c0d35741cac",
   "metadata": {},
   "source": [
    "Logistic Regression and Support Vector Machine\n",
    "\n",
    "In this task, we choose Logistic Regression and Support Vector Machine as classification models because they are well-suited for high-dimensional sparse data. Logistic Regression is an efficient linear classifier with low computational cost, making it ideal for binary classification tasks while offering good interpretability. SVM performs well in high-dimensional spaces by finding the optimal decision boundary, improving generalization, especially for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "952471fe-a541-4d71-bcac-0b5fe11ce237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f74044-4f4d-43c4-8f77-03682fa00162",
   "metadata": {},
   "source": [
    "Print out the report of these two model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfaf86c9-d0a2-401a-bb2d-54953041d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       267\n",
      "           1       0.85      0.77      0.81       267\n",
      "\n",
      "    accuracy                           0.81       534\n",
      "   macro avg       0.82      0.81      0.81       534\n",
      "weighted avg       0.82      0.81      0.81       534\n",
      "\n",
      "\n",
      "SVM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       267\n",
      "           1       0.85      0.79      0.82       267\n",
      "\n",
      "    accuracy                           0.83       534\n",
      "   macro avg       0.83      0.83      0.83       534\n",
      "weighted avg       0.83      0.83      0.83       534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLogistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nSVM Performance:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bae7b-ff0b-404f-b6a5-f0acf5e53ee4",
   "metadata": {},
   "source": [
    "According to the results, it turns out that SVM has a better performance. Thus, we could choose SVM to be our model.\n",
    "\n",
    "Next part we could try out the powerful modern text representation model -- BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65e1af8e-d728-4b33-af90-23a0da70c0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37dcbbe74dc49f98ae9a65b2481b9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xiach\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45914d9e781242da94545e6118f41316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5473e07d64b54bf5bae274887a907c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d77f18b14184c2083dda0765528eaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd17c42f9d949b8ac27d6e11cad98a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"BERT Model Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6d14e-1a96-4a94-a0ef-ce3db0703f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
