{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5f0fea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56578f6f",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761f4d2",
   "metadata": {},
   "source": [
    "Loading the synthetic dataset. The input data $D = \\{(\\vec{x_i}, y_i)\\}_{i=1}^n$ looks like:\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "\\vec{x_1}\\\\\\vec{x_2}\n",
    "\\end{pmatrix},\n",
    "Y =\\begin{pmatrix}\n",
    "y_1\\\\y_2\n",
    "\\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aacba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to edit the path, depending on where you put the files.\n",
    "data = pd.read_csv('data/a4_synthetic.csv')\n",
    "\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166d6ba",
   "metadata": {},
   "source": [
    "Training a linear regression model for this synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "111ff34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.799966113082318\n",
      "Epoch 2: MSE = 0.017392390107906882\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438456\n",
      "Epoch 5: MSE = 0.009365440968904255\n",
      "Epoch 6: MSE = 0.009366989180952537\n",
      "Epoch 7: MSE = 0.009367207398577986\n",
      "Epoch 8: MSE = 0.009367238983974492\n",
      "Epoch 9: MSE = 0.009367243704122532\n",
      "Epoch 10: MSE = 0.009367244427185763\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# We just declare the parameter tensors. Do not use nn.Linear.\n",
    "w = torch.tensor(w_init, requires_grad=True) # col vector W = (w_1; w_2)\n",
    "b = torch.tensor(b_init, requires_grad=True) # scalar\n",
    "\n",
    "eta = 1e-2\n",
    "# SGD optimizer with a learning rate of eta\n",
    "# Parameters include W and b\n",
    "opt = torch.optim.SGD([w, b], lr=eta)   \n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    sum_err = 0\n",
    "    \n",
    "    for row in range(X.shape[0]):\n",
    "        x = torch.tensor(X[[row], :]) # row vector X_i = (x1, x2)\n",
    "        y = torch.tensor(Y[[row]])\n",
    "\n",
    "        # Forward pass.\n",
    "        # compute predicted value for x\n",
    "        y_pred = w.T @ x.T + b\n",
    "        # compute squared error loss\n",
    "        err = (y - y_pred) ** 2\n",
    "        \n",
    "        # Backward and update.\n",
    "        # compute gradients and then update the model.\n",
    "        opt.zero_grad() # Get rid of previously computed gradients.\n",
    "        err.backward() #Compute the gradients.\n",
    "        opt.step() #Update the model.\n",
    "        \n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f221d",
   "metadata": {},
   "source": [
    "# Task 2, 3, 4\n",
    "\n",
    "## Computation Node\n",
    "\n",
    "Definition of computation nodes is as follows.\n",
    "\n",
    "#### Overall structure\n",
    "For the tensor calculation `z = x + y`, tensor `z` holds a `AdditionNode` with `left = x, right = y`. \n",
    "\n",
    "#### Backward function\n",
    "In the `backward()` for a certain kind of `Node`, we calculate the following:\n",
    "\n",
    "$$\n",
    "l\\_grad = \\frac{\\partial{Loss}}{\\partial{x}} = \\frac{\\partial{Loss}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}} = grad\\_output @ \\frac{\\partial{z}}{\\partial{x}}\\\\\n",
    "$$\n",
    "\n",
    "`r_grad` is simliar as above.\n",
    "\n",
    "We get `grad_output` from the function input, and calculate $\\frac{\\partial{z}}{\\partial{x}}$ according to the computation type.\n",
    "\n",
    "After we calculate the gradient for `x, y`, we invoke `propagate()` to continue backwarding to the deeper layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69a1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right):\n",
    "        # left: Tensor\n",
    "        self.left = left\n",
    "        # right: tensor | int (for power only)\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError('Unimplemented')\n",
    "\n",
    "    # Invoke backward() for left&right tensors(operands).\n",
    "    def propagate(self, l_grad, r_grad):\n",
    "        self.left.backward(l_grad)\n",
    "        # when powering, we don't need a derivative w.r.t. the exponent.\n",
    "        if isinstance(r_grad, np.ndarray):\n",
    "            self.right.backward(r_grad)\n",
    "        \n",
    "    def __repr__(self):        \n",
    "        return str(type(self))\n",
    "        \n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def backward(self, grad_output):        \n",
    "        l_grad = grad_output\n",
    "        r_grad = grad_output\n",
    "        self.propagate(l_grad, r_grad)\n",
    "        \n",
    "class SubstractionNode(Node):\n",
    "    def backward(self, grad_output): \n",
    "        l_grad = grad_output\n",
    "        r_grad = -grad_output\n",
    "        self.propagate(l_grad, r_grad)\n",
    "    \n",
    "class MatMulNode(Node):\n",
    "    def backward(self, grad_output):  \n",
    "        l_grad = grad_output @ self.right.data.T\n",
    "        r_grad = self.left.data.T @ grad_output\n",
    "        self.propagate(l_grad, r_grad)\n",
    "    \n",
    "class PowerNode(Node):\n",
    "    def backward(self, grad_output):  \n",
    "        base, exp = self.left.data, self.right\n",
    "        par_der = exp * (base ** (exp - 1))\n",
    "        l_grad = grad_output @ par_der\n",
    "        self.propagate(l_grad, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1584b",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "We construct our own `Tensor` as follows.\n",
    "\n",
    "#### Overall structure\n",
    "\n",
    "A `tensor` has a `np.ndarray` for storing data, a `Node` called `grad_fn` to record computation, and `requires_grad | grad` to store the gradient of loss function w.r.t. the tensor.\n",
    "\n",
    "#### Backward function\n",
    "\n",
    "The `backward()` for `Tensor` looks like:\n",
    "\n",
    "```python\n",
    "if tensor x is a calculated value:\n",
    "    Back propagate grad_ouput.\n",
    "else:\n",
    "    Exit recursion. Store grad if needed\n",
    "```\n",
    "\n",
    "At the very being of the backward, we want calculate $\\frac{\\partial{Loss}}{\\partial{Loss}}$ as the `grad_output`. Note that the value of a loss function is a scalar. Thus $\\frac{\\partial{Loss}}{\\partial{Loss}} = 1$. For convenient computation, we turn it into a 1x1 matrix.\n",
    "\n",
    "#### Arithmetic operation\n",
    "\n",
    "For every operation, we create the corresponding computation `Node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56be71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        #data: ndarray\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        #grad_fn: Node | None\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        #grad: None | np.ndarray\n",
    "        self.grad = None\n",
    "        \n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "    \n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()    \n",
    "    \n",
    "    # YOUR WORK WILL BE DONE BELOW\n",
    "    \n",
    "    # For Task 2:\n",
    "    \n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        # Call the helper function defined below.\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        return substraction(self, right)\n",
    "                \n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        return matrix_multiplication(self, right)\n",
    "\n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "\n",
    "    \n",
    "    # Backward computations. Will be implemented in Task 4.\n",
    "    def backward(self, grad_output=None):\n",
    "        # We first check if this tensor has a grad_fn: that is, one of the \n",
    "        # nodes that you defined in Task 3.\n",
    "        if self.grad_fn is not None:\n",
    "            # If grad_fn is defined, we have computed this tensor using some operation.\n",
    "            if grad_output is None:\n",
    "                # This is the starting point of the backward computation.\n",
    "                # This will typically be the tensor storing the output of\n",
    "                # the loss function, on which we have called .backward()\n",
    "                # in the training loop.\n",
    "\n",
    "                # Generally the value of a loss function is a scalar.\n",
    "                # Thus ∂Loss/∂Loss = 1.\n",
    "                # For convenient computation, we turn it into a 1x1 matrix\n",
    "                self.grad_fn.backward(np.eye(1))\n",
    "            else:\n",
    "                # This is an intermediate node in the computational graph.                \n",
    "                # This corresponds to any intermediate computation, such as\n",
    "                # a hidden layer.\n",
    "                self.grad_fn.backward(grad_output)\n",
    "        else:\n",
    "            # If grad_fn is not defined, this is an endpoint in the computational\n",
    "            # graph: learnable model parameters or input data.\n",
    "            if self.requires_grad:\n",
    "                # This tensor *requires* a gradient to be computed. This will\n",
    "                # typically be a tensor that holds learnable parameters.\n",
    "                self.grad = grad_output\n",
    "            else:\n",
    "                # This tensor *does not require* a gradient to be computed. This \n",
    "                # will typically be a tensor holding input data.\n",
    "                pass\n",
    "\n",
    "        \n",
    "# A small utility where we simply create a Tensor object. We use this to \n",
    "# mimic torch.tensor.\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "        \n",
    "# We define helper functions to implement the various arithmetic operations.\n",
    "\n",
    "# This function takes two tensors as input, and returns a new tensor holding\n",
    "# the result of an element-wise addition on the two input tensors.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data \n",
    "    grad_fn = AdditionNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def substraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    grad_fn = SubstractionNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "     \n",
    "def matrix_multiplication(left, right):\n",
    "    new_data = left.data @ right.data\n",
    "    grad_fn = MatMulNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "    \n",
    "def power(left, right): # left = base, and right = exp\n",
    "    new_data = left.data ** right\n",
    "    grad_fn = PowerNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0f04c",
   "metadata": {},
   "source": [
    "## Sanity Checks for Task 2, 3, 4\n",
    "\n",
    "Some sanity checks for Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2014827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2 ** 2\n",
    "test_matmul = x1 @ w\n",
    "\n",
    "print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n",
    "print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n",
    "print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n",
    "print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n",
    "assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n",
    "assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n",
    "assert(np.allclose(test_matmul.data, np.array([[1.6]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1bb77-e869-4e08-8996-3674eed101e6",
   "metadata": {},
   "source": [
    "Sanity check for Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f3276aba-4def-421b-b12e-bf0d7120f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AdditionNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a9bfb-ea55-4bce-9356-4956316e1904",
   "metadata": {},
   "source": [
    "Sanity check for Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "32687661-a67d-4bef-9a90-7dabb93380a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w \n",
    "diff = model_out - y\n",
    "loss = diff ** 2\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient of loss w.r.t. w =\\n', w.grad)\n",
    "\n",
    "assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n",
    "assert(x.grad is None)\n",
    "assert(y.grad is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cc295",
   "metadata": {},
   "source": [
    "An equivalent cell using PyTorch code. Your implementation should give the same result for `w.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cabcc94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6000],\n",
       "        [8.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n",
    "pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "pt_y = torch.tensor(np.array([[0.2]]))\n",
    "\n",
    "pt_model_out = pt_x @ pt_w \n",
    "pt_model_out.retain_grad() # Keep the gradient of intermediate nodes for debugging.\n",
    "\n",
    "pt_diff = pt_model_out - pt_y\n",
    "pt_diff.retain_grad()\n",
    "\n",
    "pt_loss = pt_diff ** 2\n",
    "pt_loss.retain_grad()\n",
    "\n",
    "pt_loss.backward()\n",
    "pt_w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5439b",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bc5b1",
   "metadata": {},
   "source": [
    "We could make class SGD heritat from class Optimizer. Then, according to Gradient Descent formula,\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "for each parameter, if its gradient is not None, update the parameter according to the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b03a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "        \n",
    "    def step(self):        \n",
    "        raise NotImplementedError('Unimplemented')      \n",
    "        \n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640d1db",
   "metadata": {},
   "source": [
    "For each epoch, every sample undergoes forward propagation, loss computation, backpropagation, and parameter updating, while the losses of all samples are accumulated to compute and output the average loss (MSE) at the end of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c32c7099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999661130823179\n",
      "Epoch 2: MSE = 0.017392390107906875\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438458\n",
      "Epoch 5: MSE = 0.009365440968904258\n",
      "Epoch 6: MSE = 0.009366989180952535\n",
      "Epoch 7: MSE = 0.009367207398577987\n",
      "Epoch 8: MSE = 0.00936723898397449\n",
      "Epoch 9: MSE = 0.009367243704122534\n",
      "Epoch 10: MSE = 0.009367244427185761\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/a4_synthetic.csv')\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()\n",
    "\n",
    "np.random.seed(1)\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "w = tensor(w_init, requires_grad=True)\n",
    "b = tensor(b_init, requires_grad=True)\n",
    "\n",
    "eta = 1e-2 \n",
    "opt = SGD([w, b], lr=eta) \n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    sum_err = 0\n",
    "    \n",
    "    for row in range(X.shape[0]):\n",
    "        x = tensor(X[[row], :]) \n",
    "        y = tensor(Y[[row]])\n",
    "\n",
    "        y_pred = x @ w + b  \n",
    "        err = (y - y_pred) ** 2  \n",
    "\n",
    "        opt.zero_grad()  \n",
    "        err.backward() \n",
    "        opt.step()  \n",
    "        \n",
    "        sum_err += err.item() \n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa831c",
   "metadata": {},
   "source": [
    "As expected, the result is `almost identical` to that of task 1.\n",
    "\n",
    "These results indicate that the mean squared error (MSE) dropped dramatically from epoch 1 to epoch 2 and then quickly converged to a very low value (around 0.009367) from epoch 3 onward. This suggests that the model rapidly learned the underlying relationship in the data and reached convergence, with the training error stabilizing at a low level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bef171",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3d492",
   "metadata": {},
   "source": [
    "In this part, we leveraged the custom Tensor and automatic differentiation framework developed in Questions 1-4, and extended it with additional operations to implement a binary cross-entropy loss function. We then built a two-layer feedforward neural network, trained it using our custom SGD optimizer, selected the best model based on validation performance, and finally evaluated its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fb31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edca414",
   "metadata": {},
   "source": [
    "To update the Tensor class, add a **transposed tensor `T`** that supports gradient propagation, and convert the method into a class property so that it can be easily accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0170fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "    \n",
    "    def item(self):\n",
    "        return self.data.item()    \n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return Tensor(self.data.T, requires_grad=self.requires_grad, grad_fn=TransposeNode(self))\n",
    "    \n",
    "    def __add__(self, right):\n",
    "        return addition(self, right)\n",
    "\n",
    "    def __sub__(self, right):\n",
    "        return substraction(self, right)\n",
    "                \n",
    "    def __matmul__(self, right):\n",
    "        return matrix_multiplication(self, right)\n",
    "\n",
    "    def __pow__(self, right):\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "    \n",
    "    def __mul__(self, right):\n",
    "        return multiply(self, right)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return neg(self)\n",
    "    \n",
    "    def backward(self, grad_output=None):\n",
    "        if self.grad_fn is not None:\n",
    "            if grad_output is None:\n",
    "                self.grad_fn.backward(np.eye(1))\n",
    "            else:\n",
    "                self.grad_fn.backward(grad_output)\n",
    "        else:\n",
    "            if self.requires_grad:\n",
    "                self.grad = grad_output\n",
    "                \n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d15fed",
   "metadata": {},
   "source": [
    "In the computation graph node definition, a new node is added to support the **negation (unary minus) operation**, which is necessary because when a tensor is negated, the operation must be recorded so that during backpropagation, its gradient is multiplied by -1 to ensure correct gradient propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c86b6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegNode(Node):\n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(-grad_output)\n",
    "\n",
    "\n",
    "class TransposeNode(Node):\n",
    "    def __init__(self, left):\n",
    "        super().__init__(left, None)\n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output.T)\n",
    "    def __repr__(self):\n",
    "        return \"TransposeNode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa898c4",
   "metadata": {},
   "source": [
    "## Extension for `Tensor` and `Node`\n",
    "\n",
    "Implementing **scalar (element-wise) multiplication**, the **tanh** and **sigmoid** activation functions, and the **log** function, each with its corresponding backward propagation node. These operations are then combined to form the binary cross-entropy loss function for a single sample, enabling proper gradient propagation during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d29be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyNode(Node):\n",
    "    def backward(self, grad_output):\n",
    "        l_grad = grad_output * self.right.data\n",
    "        r_grad = grad_output * self.left.data\n",
    "        self.propagate(l_grad, r_grad)\n",
    "\n",
    "class TanhNode(Node):\n",
    "    def backward(self, grad_output):\n",
    "        local_grad = 1 - np.tanh(self.left.data) ** 2\n",
    "        self.left.backward(grad_output * local_grad)\n",
    "\n",
    "def tanh(x):\n",
    "    new_data = np.tanh(x.data)\n",
    "    grad_fn = TanhNode(x, None)\n",
    "    return Tensor(new_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "class SigmoidNode(Node):\n",
    "    def backward(self, grad_output):\n",
    "        s = 1 / (1 + np.exp(-self.left.data))\n",
    "        local_grad = s * (1 - s)\n",
    "        self.left.backward(grad_output * local_grad)\n",
    "\n",
    "def sigmoid(x):\n",
    "    new_data = 1 / (1 + np.exp(-x.data))\n",
    "    grad_fn = SigmoidNode(x, None)\n",
    "    return Tensor(new_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "class LogNode(Node):\n",
    "    def backward(self, grad_output):\n",
    "        local_grad = 1 / self.left.data\n",
    "        self.left.backward(grad_output * local_grad)\n",
    "\n",
    "def log(x):\n",
    "    new_data = np.log(x.data)\n",
    "    grad_fn = LogNode(x, None)\n",
    "    return Tensor(new_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "def bce_loss(pred, target):\n",
    "    one = tensor(np.ones_like(pred.data))\n",
    "    l1 = target * log(pred)\n",
    "    l2 = (one - target) * log(one - pred)\n",
    "    loss = -(l1 + l2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57cb65",
   "metadata": {},
   "source": [
    "## Model training with hyperparameter search\n",
    "\n",
    "The structure of the feedfoward neural network we used is the same as the one mentioned in the instructions:\n",
    "\n",
    "1. Feature learning: 1 hidden layer, tanh activation, with `hidden_dim` neurons.\n",
    "\n",
    "2. Classification: Logistic regression, cross-entropy loss with SGD optimization, learning rate `eta`.\n",
    "\n",
    "Other hyperparameters that are not related to network structure:\n",
    "\n",
    "1. Training for `max_epoch = 50` epochs. Early stopping with `patience = 1` (number of epochs to tolerate validation accuracy decrease).\n",
    "\n",
    "In the following part, we search in a hyperparameter space, and find the best `(hidden_dim, eta)`. As a result, we reached the best validation accuracy of `80.00%` at `(14, 0.0001)`, and the corresponding test accuracy is `82.22%`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e9b8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hidden_dim=3, lr=0.01): Accuracy = 0.688889 at epoch: 2\n",
      "(hidden_dim=3, lr=0.001): Accuracy = 0.500000 at epoch: 49\n",
      "(hidden_dim=3, lr=0.0001): Accuracy = 0.766667 at epoch: 8\n",
      "(hidden_dim=3, lr=1e-05): Accuracy = 0.711111 at epoch: 49\n",
      "(hidden_dim=7, lr=0.01): Accuracy = 0.744444 at epoch: 2\n",
      "(hidden_dim=7, lr=0.001): Accuracy = 0.755556 at epoch: 8\n",
      "(hidden_dim=7, lr=0.0001): Accuracy = 0.800000 at epoch: 16\n",
      "(hidden_dim=7, lr=1e-05): Accuracy = 0.688889 at epoch: 6\n",
      "(hidden_dim=14, lr=0.01): Accuracy = 0.777778 at epoch: 4\n",
      "(hidden_dim=14, lr=0.001): Accuracy = 0.777778 at epoch: 31\n",
      "(hidden_dim=14, lr=0.0001): Accuracy = 0.800000 at epoch: 3\n",
      "(hidden_dim=14, lr=1e-05): Accuracy = 0.600000 at epoch: 49\n",
      "(hidden_dim=21, lr=0.01): Accuracy = 0.777778 at epoch: 2\n",
      "(hidden_dim=21, lr=0.001): Accuracy = 0.711111 at epoch: 7\n",
      "(hidden_dim=21, lr=0.0001): Accuracy = 0.700000 at epoch: 36\n",
      "(hidden_dim=21, lr=1e-05): Accuracy = 0.377778 at epoch: 11\n",
      "(hidden_dim=35, lr=0.01): Accuracy = 0.688889 at epoch: 4\n",
      "(hidden_dim=35, lr=0.001): Accuracy = 0.600000 at epoch: 11\n",
      "(hidden_dim=35, lr=0.0001): Accuracy = 0.777778 at epoch: 49\n",
      "(hidden_dim=35, lr=1e-05): Accuracy = 0.655556 at epoch: 18\n",
      "\n",
      "Best Hyperparameters:\n",
      "{'hidden_dim': 14, 'learning_rate': 0.0001}\n",
      "Best Overall Validation Accuracy: 0.8\n",
      "\n",
      "Test accuracy with the best hyperparameters: 0.8222222222222222\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Load data, and pre-processing.\n",
    "a4data = pd.read_csv('data/raisins.csv')\n",
    "X = scale(a4data.drop(columns='Class'))\n",
    "Y = (a4data.Class == 'Besni').to_numpy().astype(int)\n",
    "\n",
    "# train:test:validation = 8:1:1\n",
    "# Train-test split\n",
    "Xtrain, Xtemp, Ytrain, Ytemp = train_test_split(X, Y, random_state=114514, test_size=0.2, stratify=Y)\n",
    "# Test-validation split\n",
    "Xtest, Xval, Ytest, YVal = train_test_split(Xtemp, Ytemp, random_state=1919810, test_size=0.5, stratify=Ytemp)\n",
    "\n",
    "# Neural network structure:\n",
    "# 1. One hidden layer with tanh activation.\n",
    "# 2. Logistic regression binary classifer (output: probability of positive).\n",
    "def forward(x):\n",
    "    z1 = x @ W1 + b1\n",
    "    a1 = tanh(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    y_prob = sigmoid(z2)\n",
    "    return y_prob\n",
    "\n",
    "# Used for validation and test after training\n",
    "def predict_and_evaluate(input, output):\n",
    "    correct = 0\n",
    "    for i in range(len(input)):\n",
    "        x_sample = tensor(input[i:i+1, :])\n",
    "        y_pred = forward(x_sample)\n",
    "        # Classify input based on predicted probability of positive.\n",
    "        pred_label = 1 if y_pred.data[0, 0] >= 0.5 else 0\n",
    "        if pred_label == output[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(output)\n",
    "    return accuracy\n",
    "\n",
    "# Hyper-parameter search space\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "hidden_dims = [3, 7, 14, 21, 35]\n",
    "max_epoch = 50\n",
    "patience = 1\n",
    "np.random.seed(1919810)\n",
    "\n",
    "# Searching Result\n",
    "best_config = None\n",
    "best_accuracy = 0.0\n",
    "best_model = None\n",
    "\n",
    "# Searching hyper-parameter space\n",
    "for hidden_dim in hidden_dims:\n",
    "    for eta in learning_rates:\n",
    "        # Dim of input x and output y\n",
    "        input_dim = 7\n",
    "        output_dim = 1\n",
    "        \n",
    "        # Randomly initialied W/b for input layer -> hidden layer\n",
    "        W1_init = np.random.randn(input_dim, hidden_dim)\n",
    "        b1_init = np.random.randn(1, hidden_dim)\n",
    "        # Randomly initialied W/b for hidden layer -> output layer\n",
    "        W2_init = np.random.randn(hidden_dim, output_dim)\n",
    "        b2_init = np.random.randn(1, output_dim)\n",
    "        \n",
    "        W1 = tensor(W1_init, requires_grad=True)\n",
    "        b1 = tensor(b1_init, requires_grad=True)\n",
    "        W2 = tensor(W2_init, requires_grad=True)\n",
    "        b2 = tensor(b2_init, requires_grad=True)\n",
    "        \n",
    "        # Optimizer: SGD\n",
    "        opt = SGD([W1, b1, W2, b2], lr=eta)\n",
    "        accuracy = 0\n",
    "        accuracy_decrease_count = 0\n",
    "        # Train model with `max_epoch` epochs with early stopping.\n",
    "        for epoch in range(max_epoch):\n",
    "            loss = None\n",
    "            # Iterate all training data\n",
    "            for i in range(len(Ytrain)):\n",
    "                x_sample = tensor(Xtrain[i:i+1, :])\n",
    "                y_target = tensor(Ytrain[i:i+1].reshape(1, 1))\n",
    "                y_pred = forward(x_sample)\n",
    "                loss = bce_loss(y_pred, y_target)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            # Calculate the validation accuracy when using the given hyper-parameter\n",
    "            # (hidden-dim, learning rate eta).\n",
    "            accuracy_epoch = predict_and_evaluate(Xval, YVal)\n",
    "            # Early Stopping\n",
    "            if accuracy_epoch < accuracy:\n",
    "                accuracy_decrease_count += 1\n",
    "                if accuracy_decrease_count > patience:\n",
    "                    break\n",
    "            else:\n",
    "                accuracy = accuracy_epoch\n",
    "        # Update the best hyper-parameter and related info\n",
    "        if accuracy >= best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_config = {\"hidden_dim\": hidden_dim, \"learning_rate\": eta}\n",
    "            best_model = [W1.data.copy(), b1.data.copy(), W2.data.copy(), b2.data.copy()]\n",
    "        print(f\"(hidden_dim={hidden_dim}, lr={eta}): Accuracy = {accuracy:.6f} at epoch: {epoch}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_config)\n",
    "print(\"Best Overall Validation Accuracy:\", best_accuracy)\n",
    "\n",
    "# Test model accuracy\n",
    "W1, b1, W2, b2 = best_model\n",
    "accuracy = predict_and_evaluate(Xtest, Ytest)\n",
    "print(f\"\\nTest accuracy with the best hyperparameters: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cae78",
   "metadata": {},
   "source": [
    "## Conclusion for task 6\n",
    "\n",
    "First, through the experiments in Task 6, we demonstrated the feasibility of building a neural network model using a custom `Tensor` with automatic differentiation framework. Then, after systematically searching over various hyperparameter combinations—including hidden layer size and learning rate—we found that the best configuration is a hidden layer size of `14` with a learning rate of `0.0001`, achieving a peak validation accuracy of `80.00%`, and the corresponding test accuracy is `82.22%`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
